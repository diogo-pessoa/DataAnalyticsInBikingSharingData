{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Technical Report - Bike Share Data Analysis\n",
    "---\n",
    "## Sillhoute Score Analysis for optmizing KMeans\n",
    "\n",
    "Diogo Pessoa\n",
    "TODO - replace before submission\n",
    "{$STUDENT_ID}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['start_station_id_index', 'end_station_id_index', 'day_period_index', 'week_day_index']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# # Scaling the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# # Combine the VectorAssembler and StandardScaler into a Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# You can now define a pipeline that includes both the assembler and the scaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit and transform the DataFrame using the defined pipeline\n",
    "sampled_df_scaled = pipeline.fit(sampled_df_with_added_features_indexed).transform(\n",
    "    sampled_df_with_added_features_indexed)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_values = list(range(5, 16))\n",
    "\n",
    "# Initialize an empty list to store silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over values of k\n",
    "\n",
    "for k in k_values:\n",
    "    # Initialize KMeans with the specified number of clusters (k) and a seed for reproducibility\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model = kmeans.fit(adf_kmeans)\n",
    "\n",
    "    # Transform the dataset to include cluster predictions\n",
    "    predictions = model.transform(adf_kmeans)\n",
    "\n",
    "    # Evaluate the model\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, 'bx-', color='orange')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette score vs. Number of clusters (k)')\n",
    "plt.xticks(np.arange(min(k_values), max(k_values) + 1, 1.0))\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(images_path, 'silhouette_score_vs_number_of_clusters.png'))\n",
    "plt.show()\n",
    "\n",
    "# Output the optimal k based on silhouette score\n",
    "optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"The optimal number of clusters k is: {optimal_k}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
