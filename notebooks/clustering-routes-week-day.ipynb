{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run './feature_engineering.ipynb'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Unsupervised Learning] Predictive Analysis (Kmeans Clustering)\n",
    "\n",
    "Using the added features, I'll apply K-means clustering to find the most popular source stations by time of day.\n",
    "\n",
    "The steps involve:\n",
    "#### Data Preparation:\n",
    "##### Convert the categorical features to numeric index (StringIndexer)\n",
    "* Potentially use `OneHotEncoder`, since, we added the categorical fields for time of day and day of week, we may skip this step.\n",
    "* Vectorize the features (VectorAssembler)\n",
    "* Apply PCA to inspect the relationships between the chosen features, for correlation and variance.\n",
    "\n",
    "##### Clustering:\n",
    "\n",
    "Run an initial KMeans clustering with a default of k=5 clusters, then apply the silhouette index to look for the optimal number of clusters.\n",
    "Compare and plot results.\n",
    "\n",
    "Proceed with k-means for some specific questions:\n",
    "\n",
    "* Apply K-means clustering to find the most popular source/destinations stations - Plot\n",
    "* Apply K-means for most popular stations by time of day/day of week - Plot\n",
    "* Most popular routes by day of week - Plot\n",
    "* Most popular routes by day of week/time of day - Plot\n",
    "\n",
    "In a real-world scenario, the period of the day (Morning, Afternoon, Evening, and Night) is not granular enough. To get better accuracy in the prediction, to confidently allocate, employees to relocate bikes. We should consider shorter time windows, increasing  granularity, such as 15-minute intervals.\n",
    "\n",
    "#### [Supervised Learning] Predictive Analysis (Regression)\n",
    "\n",
    "Explore a predictive analysis using regression models to improve user experience. Refining the re-stock strategy by considering the bike type per route, to predict the number of bikes needed at each station at different times of the day.\n",
    "\n",
    "* Apply regression models to predict the number of bikes needed at each station at different times of the day.\n",
    "* Bike type per routes, to predict the number of bikes needed at each station at different times of the day.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select features to be used in the PCA analysis\n",
    "features = ['start_station_id_index', 'end_station_id_index', 'day_period_index', 'week_day_index']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# # Scaling the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# # Combine the VectorAssembler and StandardScaler into a Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# You can now define a pipeline that includes both the assembler and the scaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit and transform the DataFrame using the defined pipeline\n",
    "sampled_df_scaled = pipeline.fit(sampled_df_with_added_features_indexed).transform(\n",
    "    sampled_df_with_added_features_indexed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "adf_kmeans = sampled_df_scaled\n",
    "# Initialize KMeans with the specified number of clusters (k) and a seed for reproducibility\n",
    "kmeans = KMeans().setK(5).setSeed(1).setFeaturesCol(\"features\")\n",
    "\n",
    "# Fit the model to the data\n",
    "model = kmeans.fit(adf_kmeans)\n",
    "\n",
    "# Transform the dataset to include cluster predictions\n",
    "predictions = model.transform(adf_kmeans)\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize the evaluator with silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Evaluate the model\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score with k={k}: {silhouette}\")\n",
    "# Silhouette Score with k=5: 0.5733339010164845"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO apply K-means clustering and to find the most popular source stations - Plot\n",
    "# TODO - Can I apply K-means for most popular stations by time of day? - Plot\n",
    "# TODO - most popular routes - Plot\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
