{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed26109-681e-4def-957b-31ebf90ca858",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-03T11:07:29.254283Z",
     "start_time": "2024-02-03T11:07:29.199107Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loading GCP Big Query Client\n",
    "\"\"\"\n",
    "from google.cloud import bigquery\n",
    "\n",
    "project_id = os.environ.get('GCP_PROJECT_ID')\n",
    "table_name = os.environ.get('BG_TABLE_NAME')\n",
    "dataset_id = os.environ.get('BG_DATASET_ID')\n",
    "bg_location = os.environ.get('BG_LOCATION')\n",
    "table_id = os.environ.get('BG_TABLE_ID')\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client(project_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T11:07:30.136594Z",
     "start_time": "2024-02-03T11:07:29.249471Z"
    }
   },
   "id": "2f5eb30ed1b3b733",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33e19b8-8c5e-4e2d-8504-53cab6f3eacd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-03T11:07:47.134078Z",
     "start_time": "2024-02-03T11:07:30.082729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/macbook/code/TechProject1/venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/macbook/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/macbook/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-aea1bd3f-e61d-4ab1-973a-e0d237ee7e71;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.21.0 in central\n",
      ":: resolution report :: resolve 178ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.21.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-aea1bd3f-e61d-4ab1-973a-e0d237ee7e71\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/10ms)\n",
      "24/02/03 11:07:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[ride_id: string, rideable_type: string, started_at: timestamp, ended_at: timestamp, start_station_name: string, start_station_id: string, end_station_name: string, end_station_id: string, start_lat: float, start_lng: float, end_lat: float, end_lng: float, member_casual: string]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading Pandas to validate some checks in sample data, to optimize time.\n",
    "\"\"\"\n",
    "\n",
    "from divvy_bike_share_data_analysis.data_loader import load_dataset_to_local_fs\n",
    "from divvy_bike_share_data_analysis.utils_pyspark import load_data_to_spark_big_query_enabled\n",
    "\n",
    "import glob\n",
    "\n",
    "# Collecting data to local directory\n",
    "DATA_COLLECTION_DIR: str = '../data_collection/'\n",
    "\n",
    "# This is commented out, as I only collect the csvs in the very first run.\n",
    "if not glob.glob(os.path.join(DATA_COLLECTION_DIR, '*.csv')):\n",
    "    print('collecting data to local fs')\n",
    "    load_dataset_to_local_fs(DATA_COLLECTION_DIR, [2020, 2021, 2022, 2023])\n",
    "\n",
    "# creating pyspark session and pre-processing data\n",
    "divvy_df = load_data_to_spark_big_query_enabled(DATA_COLLECTION_DIR)\n",
    "divvy_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from google.cloud import bigquery\n",
    "# \n",
    "# schema=[\n",
    "#     bigquery.SchemaField(\"ride_id\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"rideable_type\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"started_at\", \"TIMESTAMP\"),\n",
    "#     bigquery.SchemaField(\"ended_at\", \"TIMESTAMP\"),\n",
    "#     bigquery.SchemaField(\"start_station_name\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"start_station_id\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"end_station_name\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"end_station_id\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"start_lat\", \"FLOAT\"),\n",
    "#     bigquery.SchemaField(\"start_lng\", \"FLOAT\"),\n",
    "#     bigquery.SchemaField(\"end_lat\", \"FLOAT\"),\n",
    "#     bigquery.SchemaField(\"end_lng\", \"FLOAT\"),\n",
    "#     bigquery.SchemaField(\"member_casual\", \"STRING\"),\n",
    "# ]\n",
    "# \n",
    "# table = client.get_table(table_id)  # Make an API request.\n",
    "# original_schema = table.schema\n",
    "# new_schema = original_schema[:]  # Creates a copy of the schema.\n",
    "# [new_schema.append(field) for field in schema]\n",
    "# table.schema = new_schema\n",
    "# table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T11:07:47.158669Z",
     "start_time": "2024-02-03T11:07:47.136717Z"
    }
   },
   "id": "6b9df7f95f96bb89",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70bf0e23-63c1-42c7-bf4f-0bf89e1f4efc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T11:07:49.444784Z",
     "start_time": "2024-02-03T11:07:47.149450Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.inject.ProvisionException: Unable to provision, see the following errors:\n\n1) Error in custom provider, java.lang.IllegalArgumentException: No table has been specified\n  at com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:68)\n  while locating com.google.cloud.spark.bigquery.SparkBigQueryConfig\n\n1 error\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:226)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1097)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1131)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.IllegalArgumentException: No table has been specified\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:195)\n\tat com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:70)\n\tat com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule$$FastClassByGuice$$aebd1752.invoke(<generated>)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod$FastClassProviderMethod.doProvision(ProviderMethod.java:264)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:173)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1094)\n\t... 43 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mPushing all data to BigQuery table\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mdivvy_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbigquery\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdivvy_trips\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mx-oxygen-413115.divvy_project1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1461\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1459\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1461\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o54.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.inject.ProvisionException: Unable to provision, see the following errors:\n\n1) Error in custom provider, java.lang.IllegalArgumentException: No table has been specified\n  at com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:68)\n  while locating com.google.cloud.spark.bigquery.SparkBigQueryConfig\n\n1 error\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:226)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1097)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1131)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.IllegalArgumentException: No table has been specified\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:195)\n\tat com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:70)\n\tat com.google.cloud.spark.bigquery.v2.SparkBigQueryConnectorModule$$FastClassByGuice$$aebd1752.invoke(<generated>)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod$FastClassProviderMethod.doProvision(ProviderMethod.java:264)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:173)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1094)\n\t... 43 more\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pushing all data to BigQuery table\n",
    "\"\"\"\n",
    "divvy_df.write.format('bigquery').option('divvy_trips', 'x-oxygen-413115.divvy_project1').save()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eab9cfeb2b47744a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Once I split the loaded data into a sample size, I can use pandas to validate some checks, and save time as Spark is slow locally. \n",
    "\"\"\"\n",
    "\n",
    "divvy_df = divvy_df.dropna()\n",
    "sampled_df = divvy_df.sample(withReplacement=False, fraction=0.01)\n",
    "# sampled_df.columns\n",
    "divvy_df_pandas = sampled_df.toPandas()\n",
    "from divvy_bike_share_data_analysis import bike_stations\n",
    "\n",
    "bike_stations = bike_stations.get_unique_bike_stations_ids(sampled_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be85a1fd9a73562d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grouped_stations_by_matching_id_pd = divvy_df_pandas[\n",
    "    ['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']].drop_duplicates().dropna()\n",
    "conflicting_stations = grouped_stations_by_matching_id_pd[\n",
    "    grouped_stations_by_matching_id_pd['start_station_name'] != grouped_stations_by_matching_id_pd['end_station_name']]\n",
    "\n",
    "bike_stations_pd = grouped_stations_by_matching_id_pd[[\"start_station_id\", \"start_station_name\"]].rename(\n",
    "    columns={\"start_station_id\": \"station_id\", \"start_station_name\": \"station_name\"}).drop_duplicates()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf4d30a880e36962",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from divvy_bike_share_data_analysis.bike_stations import categorize_time_of_day\n",
    "from pyspark.sql.functions import udf, hour\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Adding a categorical field to determine if the station is a start or end station\n",
    "# Convert to a UDF Function by passing in the function and return type\n",
    "\n",
    "# Before we start, check for nulls\n",
    "# sampled_df.filter(sampled_df.started_at.isNull()).count() # there's 0\n",
    "\n",
    "time_of_day_udf = udf(categorize_time_of_day, StringType())\n",
    "sampled_df_with_day_period = sampled_df.withColumn('day_period', time_of_day_udf(hour('started_at')))\n",
    "# sampled_df_with_day_period.show(5)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sampled_df_with_day_period.filter(col('day_period').isNull()).count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c28491bfcb070c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"day_period\", outputCol=\"day_period_index\")\n",
    "sampled_df_with_day_period_indexed = indexer.fit(sampled_df_with_day_period).transform(sampled_df_with_day_period)\n",
    "\n",
    "#from pyspark.ml.feature import OneHotEncoder\n",
    "# OneHotEnconding\n",
    "# \n",
    "# encoder = OneHotEncoder(inputCols=[\"day_period_index\"], outputCols=[\"day_period_ohe\"])\n",
    "# sampled_df_with_day_period_ohed = encoder.fit(sampled_df_with_day_period_indexed).transform(sampled_df_with_day_period_indexed)\n",
    "# from pyspark.sql.functions import when\n",
    "# \n",
    "# sampled_df_with_day_period_indexed.filter(col('day_period_index').isNull()).count()\n",
    "# sampled_df_with_day_period_indexed = sampled_df_with_day_period_indexed.withColumn(\"day_period_index\", when(col(\"day_period_index\").isNull(), 0.0).otherwise(col(\"day_period_index\"))) # TODO review this as potential problem. \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0da0a71359a4504",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pandas Approach:\n",
    "divvy_df_pandas.dropna(subset=['start_station_id', 'end_station_id'], inplace=True)\n",
    "divvy_df_pandas['start_station_name', 'end_station_name'] = divvy_df_pandas['start_station_id'].map(lambda x: bike_stations_pd[bike_stations_pd['station_id'] == x]['station_name'].values[0])\n",
    "divvy_df_pandas[['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']]\n",
    "\"\"\"\n",
    "sanitized_bike_stations = sampled_df_with_day_period_indexed.join(\n",
    "    bike_stations.withColumnRenamed(\"station_name\", \"start_station_name\"),\n",
    "    sampled_df_with_day_period_indexed.start_station_id == bike_stations.station_id,\n",
    "    \"left\").drop(\"station_id\")\n",
    "\n",
    "# Join to map end_station_id to end_station_name\n",
    "sanitized_bike_stations = sanitized_bike_stations.join(\n",
    "    bike_stations.withColumnRenamed(\"station_name\", \"end_station_name\"),\n",
    "    sanitized_bike_stations.end_station_id == bike_stations.station_id,\n",
    "    \"left\").drop(\"station_id\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "526923eab1a1b7dc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer_start_station_id = StringIndexer(inputCol=\"start_station_id\", outputCol=\"start_station_id_index\")\n",
    "indexer_end_station_id = StringIndexer(inputCol=\"end_station_id\", outputCol=\"end_station_id_index\")\n",
    "sanitized_bike_stations_id_indexed = indexer_start_station_id.fit(sanitized_bike_stations).transform(\n",
    "    sanitized_bike_stations)\n",
    "sanitized_bike_stations_id_indexed = indexer_end_station_id.fit(sanitized_bike_stations).transform(\n",
    "    sanitized_bike_stations_id_indexed)\n",
    "sanitized_bike_stations_id_indexed.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b005a367f1496da8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sanitized_bike_stations_id_indexed.dropna(subset=['day_period_index'])\n",
    "sanitized_bike_stations_id_indexed.dropna(subset=['start_station_id_index'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24109f00d6906fd0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# most popular stations by period of day\n",
    "feature_cols = ['start_station_id_index', 'day_period_index']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the data to have the features column\n",
    "adf_kmeans = assembler.transform(sanitized_bike_stations_id_indexed)\n",
    "### Custom spark Session to support synapsesML:\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.2\") \\\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "k = 5  # Adjust based on your data and needs\n",
    "\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(adf_kmeans)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0fe4bd6b32b3c08",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "# predictions = model.transform(adf_kmeans)\n",
    "# \n",
    "# # Group by cluster label and start_station_id, and count occurrences\n",
    "# popular_start_stations = predictions.groupBy(\"prediction\", \"start_station_id_index\")\n",
    "# \n",
    "# # Find the most popular station in each cluster\n",
    "# most_popular_start_stations = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "# \n",
    "# popular_destinations = predictions.groupBy(\"prediction\", \"end_station_id_index\").count()\n",
    "# most_popular_destination = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "\n",
    "# TODO apply K-means clustering and to find the most popular source stations - Plot\n",
    "# TODO - Can I apply K-means for most popular stations by time of day? - Plot\n",
    "# TODO - most popular routes - Plot \n",
    "\n",
    "\n",
    "predictions.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=predictions, x='start_station_id_index', y='start_lat', hue='prediction', palette='viridis')\n",
    "# plt.title('KMeans Clustering Results')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c36964280c62938c"
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
