{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:48:12.101978Z",
     "start_time": "2024-02-01T14:47:57.652930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/01 14:48:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from divvy_bike_share_data_analysis import utils_pyspark\n",
    "\n",
    "\"\"\"\n",
    "Loading Pandas to validate some checks in sample data, to optmize time.\n",
    "\"\"\"\n",
    "\n",
    "# Collecting data to local directory\n",
    "DATA_COLLECTION_DIR: str = '/Users/macbook/code/TechProject1/data_collection/'\n",
    "# data_loader.load_dataset_to_local_fs(DATA_COLLECTION_DIR, [2020, 2021, 2022, 2023])\n",
    "\n",
    "# creating pyspark session and pre-processing data\n",
    "divvy_df = utils_pyspark.load_data(DATA_COLLECTION_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "Row(ride_id='ride_id', rideable_type='rideable_type', started_at=None, ended_at=None, start_station_name='start_station_name', start_station_id='start_station_id', end_station_name='end_station_name', end_station_id='end_station_id', start_lat=None, start_lng=None, end_lat=None, end_lng=None, member_casual='member_casual')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divvy_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:48:14.872904Z",
     "start_time": "2024-02-01T14:48:12.106760Z"
    }
   },
   "id": "118638323dbf9bc3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Once I split the loaded data into a sample size, I can use pandas to validate some checks, and save time as Spark is slow locally. \n",
    "\"\"\"\n",
    "\n",
    "sampled_df = divvy_df.sample(withReplacement=False, fraction=0.01)\n",
    "# sampled_df.columns\n",
    "divvy_df_pandas = sampled_df.toPandas()\n",
    "divvy_df_pandas.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:50:05.730395Z",
     "start_time": "2024-02-01T14:48:14.865608Z"
    }
   },
   "id": "30f56f396c315724"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------+--------------------+\n",
      "|start_station_id|  start_station_name|end_station_id|    end_station_name|\n",
      "+----------------+--------------------+--------------+--------------------+\n",
      "|           13389|Clarendon Ave & J...|         13389|Clarendon Ave & J...|\n",
      "|           13154|Sheffield Ave & K...|         13154|Sheffield Ave & K...|\n",
      "|             464|Western Ave & Ard...|           464|Western Ave & Ard...|\n",
      "|           20214| Avenue O & 134th St|         20214| Avenue O & 134th St|\n",
      "|             291|Wells St & Evergr...|           291|Wells St & Evergr...|\n",
      "|             344|Ravenswood Ave & ...|           344|Ravenswood Ave & ...|\n",
      "|           13277|Broadway & Belmon...|         13277|Broadway & Belmon...|\n",
      "|    KA1503000004|Lake Park Ave & 3...|  KA1503000004|Lake Park Ave & 3...|\n",
      "|          RP-001|    Warren Park West|        RP-001|    Warren Park West|\n",
      "|             170|Clinton St & 18th St|           170|Clinton St & 18th St|\n",
      "|             497|Kimball Ave & Bel...|           497|Kimball Ave & Bel...|\n",
      "|    KA1504000096|Sawyer Ave & Irvi...|  KA1504000096|Sawyer Ave & Irvi...|\n",
      "|             602|Central St & Gira...|           602|Central St & Gira...|\n",
      "|             430| MLK Jr Dr & 63rd St|           430| MLK Jr Dr & 63rd St|\n",
      "|           15632|Talman Ave & Addi...|         15632|Talman Ave & Addi...|\n",
      "|           13061|Ashland Ave & Div...|         13061|Ashland Ave & Div...|\n",
      "|    KA1503000033|Eberhart Ave & 61...|  KA1503000033|Eberhart Ave & 61...|\n",
      "|             657|Wood St & Augusta...|           657|Wood St & Augusta...|\n",
      "|             305|Western Ave & Div...|           305|Western Ave & Div...|\n",
      "|    TA1307000113|Southport Ave & W...|  TA1307000113|Southport Ave & W...|\n",
      "+----------------+--------------------+--------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from divvy_bike_share_data_analysis import bike_stations\n",
    "bike_stations = bike_stations.get_unique_bike_stations_ids(sampled_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:51:49.591335Z",
     "start_time": "2024-02-01T14:50:05.893568Z"
    }
   },
   "id": "a634e7cafb4c0a34"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Pandas approach\n",
    "grouped_stations_by_matching_id_pd = divvy_df_pandas[\n",
    "    ['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']].drop_duplicates().dropna()\n",
    "conflicting_stations = grouped_stations_by_matching_id_pd[\n",
    "    grouped_stations_by_matching_id_pd['start_station_name'] != grouped_stations_by_matching_id_pd['end_station_name']]\n",
    "\n",
    "bike_stations_pd = grouped_stations_by_matching_id_pd[[\"start_station_id\", \"start_station_name\"]].rename(\n",
    "    columns={\"start_station_id\": \"station_id\", \"start_station_name\": \"station_name\"}).drop_duplicates()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:51:50.804666Z",
     "start_time": "2024-02-01T14:51:49.606714Z"
    }
   },
   "id": "307641a0fe3a0c8b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# At this point we know that trips without Ids, are not relevant for our analysis. So we can drop them\n",
    "sampled_df = sampled_df.dropna(subset=['start_station_id', 'end_station_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:58:43.925653Z",
     "start_time": "2024-02-01T14:58:43.782739Z"
    }
   },
   "id": "96f9a7acecb7697f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "New categorical field day_period based on started_at\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0cea6253b615555"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from divvy_bike_share_data_analysis.bike_stations import categorize_time_of_day\n",
    "from pyspark.sql.functions import udf, hour\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Adding a categorical field to determine if the station is a start or end station\n",
    "# Convert to a UDF Function by passing in the function and return type\n",
    "\n",
    "# Before we start, check for nulls\n",
    "# sampled_df.filter(sampled_df.started_at.isNull()).count() # there's 0\n",
    "\n",
    "time_of_day_udf = udf(categorize_time_of_day, StringType())\n",
    "sampled_df_with_day_period = sampled_df.withColumn('day_period', time_of_day_udf(hour('started_at')))\n",
    "# sampled_df_with_day_period.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:58:55.407864Z",
     "start_time": "2024-02-01T14:58:55.236608Z"
    }
   },
   "id": "7009d985a9834288"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sampled_df_with_day_period.filter(col('day_period').isNull()).count() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:38:37.727111Z",
     "start_time": "2024-02-01T15:37:33.940677Z"
    }
   },
   "id": "140b110fc3064518"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"day_period\", outputCol=\"day_period_index\")\n",
    "sampled_df_with_day_period_indexed = indexer.fit(sampled_df_with_day_period).transform(sampled_df_with_day_period)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:23:35.955373Z",
     "start_time": "2024-02-01T15:21:42.560416Z"
    }
   },
   "id": "2841a0f29a4aa8db"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "sampled_df_with_day_period_indexed.filter(col('day_period_index').isNull()).count()\n",
    "sampled_df_with_day_period_indexed = sampled_df_with_day_period_indexed.withColumn(\"day_period_index\", when(col(\"day_period_index\").isNull(), 0.0).otherwise(col(\"day_period_index\"))) # TODO review this as potential problem. \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:54:57.115836Z",
     "start_time": "2024-02-01T15:54:55.754099Z"
    }
   },
   "id": "d9038532f2fead11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:======>                                                  (4 + 1) / 33]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# OneHotEnconding\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"day_period_index\"], outputCols=[\"day_period_ohe\"])\n",
    "sampled_df_with_day_period_ohed = encoder.fit(sampled_df_with_day_period_indexed).transform(sampled_df_with_day_period_indexed)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-01T15:55:03.431059Z"
    }
   },
   "id": "4215e281019568cc"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "sanitized_bike_stations = sampled_df_with_day_period_ohed.join(\n",
    "    bike_stations.withColumnRenamed(\"station_name\", \"start_station_name\"),\n",
    "    sampled_df_with_day_period_ohed.start_station_id == bike_stations.station_id,\n",
    "    \"left\").drop(\"station_id\")\n",
    "\n",
    "# Join to map end_station_id to end_station_name\n",
    "sanitized_bike_stations = sanitized_bike_stations.join(\n",
    "    bike_stations.withColumnRenamed(\"station_name\", \"end_station_name\"),\n",
    "    sanitized_bike_stations.end_station_id == bike_stations.station_id,\n",
    "    \"left\").drop(\"station_id\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:01:54.445741Z",
     "start_time": "2024-02-01T15:01:53.502377Z"
    }
   },
   "id": "defdfc2612a6b51b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc009e565e8aabd"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[ride_id: string, rideable_type: string, started_at: timestamp, ended_at: timestamp, start_station_name: string, start_station_id: string, end_station_name: string, end_station_id: string, start_lat: float, start_lng: float, end_lat: float, end_lng: float, member_casual: string, day_period: string, start_station_name: string, end_station_name: string]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitized_bike_stations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:17:44.808043Z",
     "start_time": "2024-02-01T14:17:44.778939Z"
    }
   },
   "id": "f6424fd63e97d0a7"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Pandas Approach:\n",
    "divvy_df_pandas.dropna(subset=['start_station_id', 'end_station_id'], inplace=True)\n",
    "divvy_df_pandas['start_station_name', 'end_station_name'] = divvy_df_pandas['start_station_id'].map(lambda x: bike_stations_pd[bike_stations_pd['station_id'] == x]['station_name'].values[0])\n",
    "divvy_df_pandas[['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:21:53.257395Z",
     "start_time": "2024-02-01T12:20:18.946739Z"
    }
   },
   "id": "17f30e384cb2d2cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahead of running Kmeans I need to run transformations to convert the data to a format that Kmeans can use\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9b3ff1a1e272cf3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+----------+----------------+--------------+--------------------+--------------------+----------------------+--------------------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|day_period|day_period_index|day_period_ohe|  start_station_name|    end_station_name|start_station_id_index|end_station_id_index|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+----------+----------------+--------------+--------------------+--------------------+----------------------+--------------------+\n",
      "|8FFE9593B78C6A93|electric_bike|2022-08-04 18:29:42|2022-08-04 19:04:24|Kedzie Ave & Lake St|    KA1504000106|Kedzie Ave & Lake St|  KA1504000106|41.884823| -87.70636|  41.8846| -87.70631|       casual|   Evening|             1.0| (3,[1],[1.0])|Kedzie Ave & Lake St|Kedzie Ave & Lake St|                 807.0|               888.0|\n",
      "|B1419F643D7BC556|electric_bike|2022-08-14 19:16:28|2022-08-14 19:35:58|Kedzie Ave & Lake St|    KA1504000106|California Ave & ...|         13084|41.884724| -87.70635|41.922695| -87.69715|       casual|   Evening|             1.0| (3,[1],[1.0])|Kedzie Ave & Lake St|California Ave & ...|                 807.0|               160.0|\n",
      "|443F997245713D3F| classic_bike|2022-08-20 03:14:10|2022-08-20 03:44:18|  Daley Center Plaza|    TA1306000010|Leavitt St & Nort...|  TA1308000005| 41.88424| -87.62963| 41.91051| -87.68239|       member|     Night|             3.0|     (3,[],[])|  Daley Center Plaza|Leavitt St & Nort...|                  78.0|               271.0|\n",
      "|81DE2A1FFBB46CC5|electric_bike|2022-08-31 10:43:28|2022-08-31 11:05:06|Southport Ave & R...|           13071|Dearborn Pkwy & D...|  TA1307000128| 41.94362| -87.66391|41.898968| -87.62991|       member|   Morning|             2.0| (3,[2],[1.0])|Southport Ave & R...|Dearborn Pkwy & D...|                  67.0|                49.0|\n",
      "|D98EFDA530F809DD| classic_bike|2022-08-13 15:42:21|2022-08-13 15:51:50|Ashland Ave & Bla...|           13224|Stave St & Armita...|         13266|41.907066| -87.66725| 41.91774| -87.69139|       member| Afternoon|             0.0| (3,[0],[1.0])|Ashland Ave & Bla...|Stave St & Armita...|                 209.0|               360.0|\n",
      "|124F05B14AEA0D93|electric_bike|2022-08-05 12:41:17|2022-08-05 13:04:56|Halsted St & 64th St|             744| Evans Ave & 75th St|           570|    41.78|    -87.64| 41.75849| -87.60641|       casual| Afternoon|             0.0| (3,[0],[1.0])|Halsted St & 64th St| Evans Ave & 75th St|                1315.0|              1101.0|\n",
      "|12C5545EF83D6EC6| classic_bike|2022-08-19 18:13:19|2022-08-19 18:20:06|Michigan Ave & Pe...|           13034|Franklin St & Chi...|         13017| 41.89766| -87.62351|41.896748|-87.635666|       member|   Evening|             1.0| (3,[1],[1.0])|Michigan Ave & Pe...|Franklin St & Chi...|                 173.0|               111.0|\n",
      "|B67CAEFBFD8C7931|electric_bike|2022-08-28 12:52:01|2022-08-28 12:58:44|Lakeview Ave & Fu...|    TA1309000019|Wilton Ave & Dive...|  TA1306000014|41.925808| -87.63897| 41.93242|  -87.6527|       member| Afternoon|             0.0| (3,[0],[1.0])|Lakeview Ave & Fu...|Wilton Ave & Dive...|                  39.0|                74.0|\n",
      "|B8F3297DCAF28182|electric_bike|2022-08-09 17:49:26|2022-08-09 17:58:53|Lakeview Ave & Fu...|    TA1309000019|Wilton Ave & Dive...|  TA1306000014|41.926094| -87.63878| 41.93242|  -87.6527|       casual|   Evening|             1.0| (3,[1],[1.0])|Lakeview Ave & Fu...|Wilton Ave & Dive...|                  39.0|                74.0|\n",
      "|04103402E022C380|electric_bike|2022-08-01 16:26:13|2022-08-01 16:37:30|Clark St & Columb...|          RP-008|Clark St & Berwyn...|  KA1504000146|42.004494|-87.672356| 41.97803|-87.668564|       casual| Afternoon|             0.0| (3,[0],[1.0])|Clark St & Columb...|Clark St & Berwyn...|                 594.0|               180.0|\n",
      "|13586C1878F73329|electric_bike|2022-08-11 06:54:02|2022-08-11 06:57:45|Broadway & Barry Ave|           13137|DuSable Lake Shor...|  TA1309000049|41.937614| -87.64406|41.940777| -87.63919|       member|   Morning|             2.0| (3,[2],[1.0])|Broadway & Barry Ave|Lake Shore Dr & B...|                  15.0|                 5.0|\n",
      "|13586C1878F73329|electric_bike|2022-08-11 06:54:02|2022-08-11 06:57:45|Broadway & Barry Ave|           13137|DuSable Lake Shor...|  TA1309000049|41.937614| -87.64406|41.940777| -87.63919|       member|   Morning|             2.0| (3,[2],[1.0])|Broadway & Barry Ave|DuSable Lake Shor...|                  15.0|                 5.0|\n",
      "|D3F145EAA91AEB45| classic_bike|2022-08-25 08:42:26|2022-08-25 08:53:31|Sedgwick St & Hur...|    TA1307000062|Green St & Washin...|         13053|41.894665|-87.638435|41.883183| -87.64873|       member|   Morning|             2.0| (3,[2],[1.0])|Sedgwick St & Hur...|Green St & Randol...|                 138.0|                25.0|\n",
      "|D3F145EAA91AEB45| classic_bike|2022-08-25 08:42:26|2022-08-25 08:53:31|Sedgwick St & Hur...|    TA1307000062|Green St & Washin...|         13053|41.894665|-87.638435|41.883183| -87.64873|       member|   Morning|             2.0| (3,[2],[1.0])|Sedgwick St & Hur...|Green St & Washin...|                 138.0|                25.0|\n",
      "|B2156099D39E1F0F|electric_bike|2022-08-11 19:02:44|2022-08-11 19:08:46|Ravenswood Ave & ...|    TA1309000066|Clark St & Berwyn...|  KA1504000146|41.969063| -87.67429| 41.97803|-87.668564|       member|   Evening|             1.0| (3,[1],[1.0])|Ravenswood Ave & ...|Clark St & Berwyn...|                 121.0|               180.0|\n",
      "|5C0C1E935FAF1580|electric_bike|2022-08-31 14:33:48|2022-08-31 14:56:24|Leavitt St & Addi...|    KA1504000143|Clark St & Berwyn...|  KA1504000146| 41.94654|-87.683395| 41.97803|-87.668564|       member| Afternoon|             0.0| (3,[0],[1.0])|Leavitt St & Addi...|Clark St & Berwyn...|                 559.0|               180.0|\n",
      "|DC6FD60DE5D3AB49| classic_bike|2022-08-13 14:14:28|2022-08-13 15:35:19|Lakeview Ave & Fu...|    TA1309000019|Wilton Ave & Dive...|  TA1306000014|41.925858| -87.63897| 41.93242|  -87.6527|       casual| Afternoon|             0.0| (3,[0],[1.0])|Lakeview Ave & Fu...|Wilton Ave & Dive...|                  39.0|                74.0|\n",
      "|2AA92EE82F0BC770| classic_bike|2022-08-26 08:18:42|2022-08-26 08:50:18|Lakeview Ave & Fu...|    TA1309000019|LaSalle St & Jack...|  TA1309000004|41.925858| -87.63897|41.878166| -87.63193|       casual|   Morning|             2.0| (3,[2],[1.0])|Lakeview Ave & Fu...|LaSalle St & Jack...|                  39.0|               133.0|\n",
      "|37B705907598FDA2|  docked_bike|2022-08-15 17:51:29|2022-08-15 18:55:18|DuSable Lake Shor...|           13300|Wabash Ave & Roos...|  TA1305000002| 41.88096|-87.616745|41.867226| -87.62596|       casual|   Evening|             1.0| (3,[1],[1.0])|Lake Shore Dr & M...|Wabash Ave & Roos...|                   0.0|                52.0|\n",
      "|37B705907598FDA2|  docked_bike|2022-08-15 17:51:29|2022-08-15 18:55:18|DuSable Lake Shor...|           13300|Wabash Ave & Roos...|  TA1305000002| 41.88096|-87.616745|41.867226| -87.62596|       casual|   Evening|             1.0| (3,[1],[1.0])|DuSable Lake Shor...|Wabash Ave & Roos...|                   0.0|                52.0|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+----------+----------------+--------------+--------------------+--------------------+----------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer_start_station_id = StringIndexer(inputCol=\"start_station_id\", outputCol=\"start_station_id_index\")\n",
    "indexer_end_station_id = StringIndexer(inputCol=\"end_station_id\", outputCol=\"end_station_id_index\")\n",
    "sanitized_bike_stations_id_indexed = indexer_start_station_id.fit(sanitized_bike_stations).transform(\n",
    "    sanitized_bike_stations)\n",
    "sanitized_bike_stations_id_indexed = indexer_end_station_id.fit(sanitized_bike_stations).transform(\n",
    "    sanitized_bike_stations_id_indexed)\n",
    "sanitized_bike_stations_id_indexed.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:08:18.020603Z",
     "start_time": "2024-02-01T15:02:04.192793Z"
    }
   },
   "id": "c65c27fbf4750036"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# most popular stations by period of day\n",
    "feature_cols = ['start_station_id_index', 'day_period_ohe']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the data to have the features column\n",
    "adf_kmeans = assembler.transform(sanitized_bike_stations_id_indexed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:10:03.575410Z",
     "start_time": "2024-02-01T15:10:03.459939Z"
    }
   },
   "id": "8bd7c8ee1fbc2b62"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/01 15:10:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/02/01 15:16:53 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/macbook/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/macbook/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:968)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/02/01 15:16:53 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/02/01 15:16:53 WARN BlockManager: Putting block rdd_163_0 failed due to exception java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null.\n",
      "24/02/01 15:16:53 WARN BlockManager: Block rdd_163_0 could not be removed as it was not found on disk or in memory\n",
      "24/02/01 15:16:53 ERROR Executor: Exception in task 0.0 in stage 49.0 (TID 482)\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/02/01 15:16:53 WARN TaskSetManager: Lost task 0.0 in stage 49.0 (TID 482) (192.168.24.228 executor driver): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/02/01 15:16:53 ERROR TaskSetManager: Task 0 in stage 49.0 failed 1 times; aborting job\n",
      "24/02/01 15:16:53 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 482) (192.168.24.228 executor driver): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeSample$1(RDD.scala:626)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:615)\n",
      "\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:400)\n",
      "\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)\n",
      "\tat org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/02/01 15:16:53 WARN BlockManager: Putting block rdd_163_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/02/01 15:16:53 WARN BlockManager: Block rdd_163_1 could not be removed as it was not found on disk or in memory\n",
      "24/02/01 15:16:53 WARN TaskSetManager: Lost task 1.0 in stage 49.0 (TID 483) (192.168.24.228 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 482) (192.168.24.228 executor driver): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o468.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 482) (192.168.24.228 executor driver): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeSample$1(RDD.scala:626)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:615)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:400)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)\n\tat org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)\n\tat org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)\n\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m  \u001B[38;5;66;03m# Adjust based on your data and needs\u001B[39;00m\n\u001B[1;32m     13\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans()\u001B[38;5;241m.\u001B[39msetK(k)\u001B[38;5;241m.\u001B[39msetSeed(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msetFeaturesCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43madf_kmeans\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# predictions = model.transform(adf_kmeans)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# TODO - Can I apply K-means for most popular stations by time of day? - Plot\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# TODO - most popular routes - Plot \u001B[39;00m\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/code/TechProject1/venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o468.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 482) (192.168.24.228 executor driver): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.getBaseObject()\" because \"input\" is null\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeSample$1(RDD.scala:626)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:615)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:400)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:273)\n\tat org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)\n\tat org.apache.spark.ml.clustering.KMeans.trainWithRow(KMeans.scala:446)\n\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:382)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:371)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
     ]
    }
   ],
   "source": [
    "### Custom spark Session to support synapsesML:\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    ".config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.2\") \\\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "k = 5  # Adjust based on your data and needs\n",
    "\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(adf_kmeans)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:16:55.509304Z",
     "start_time": "2024-02-01T15:10:47.337742Z"
    }
   },
   "id": "34135968fbeb541e"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "['ride_id',\n 'rideable_type',\n 'started_at',\n 'ended_at',\n 'start_station_name',\n 'start_station_id',\n 'end_station_name',\n 'end_station_id',\n 'start_lat',\n 'start_lng',\n 'end_lat',\n 'end_lng',\n 'member_casual',\n 'start_station_name',\n 'end_station_name',\n 'start_station_id_index',\n 'end_station_id_index',\n 'features',\n 'prediction']"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "# predictions = model.transform(adf_kmeans)\n",
    "# \n",
    "# # Group by cluster label and start_station_id, and count occurrences\n",
    "# popular_start_stations = predictions.groupBy(\"prediction\", \"start_station_id_index\")\n",
    "# \n",
    "# # Find the most popular station in each cluster\n",
    "# most_popular_start_stations = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "# \n",
    "# popular_destinations = predictions.groupBy(\"prediction\", \"end_station_id_index\").count()\n",
    "# most_popular_destination = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "\n",
    "# TODO apply K-means clustering and to find the most popular source stations - Plot\n",
    "# TODO - Can I apply K-means for most popular stations by time of day? - Plot\n",
    "# TODO - most popular routes - Plot \n",
    "\n",
    "\n",
    "\n",
    "predictions.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=predictions, x='start_station_id_index', y='start_lat', hue='prediction', palette='viridis')\n",
    "# plt.title('KMeans Clustering Results')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T13:50:42.626030Z",
     "start_time": "2024-02-01T13:50:42.585197Z"
    }
   },
   "id": "10ee482271dbcc7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c472533af3870c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
