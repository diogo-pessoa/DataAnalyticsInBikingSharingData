{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Technical Report - Divvy Bike Share Data Analysis\n",
    "----\n",
    "Diogo Pessoa\n",
    "TODO - replace before submission\n",
    "{$STUDENT_ID}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eee107b2a5efea28"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed26109-681e-4def-957b-31ebf90ca858",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-06T12:39:54.062471Z",
     "start_time": "2024-02-06T12:39:54.054200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "images_path = os.getenv('IMAGES_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c4986fb3a15cb4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33e19b8-8c5e-4e2d-8504-53cab6f3eacd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-06T12:41:35.172591Z",
     "start_time": "2024-02-06T12:40:08.730686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data to local fs\n",
      "Downloading files...\n",
      "Requesting file: 202301-divvy-tripdata.zip\n",
      "Error downloading file status code: 202301-divvy-tripdata.zip, 200\n",
      "Requesting file: 202302-divvy-tripdata.zip\n",
      "Error downloading file status code: 202302-divvy-tripdata.zip, 200\n",
      "Requesting file: 202303-divvy-tripdata.zip\n",
      "Error downloading file status code: 202303-divvy-tripdata.zip, 200\n",
      "Requesting file: 202304-divvy-tripdata.zip\n",
      "Error downloading file status code: 202304-divvy-tripdata.zip, 200\n",
      "Requesting file: 202305-divvy-tripdata.zip\n",
      "Error downloading file status code: 202305-divvy-tripdata.zip, 200\n",
      "Requesting file: 202306-divvy-tripdata.zip\n",
      "Error downloading file status code: 202306-divvy-tripdata.zip, 200\n",
      "Requesting file: 202307-divvy-tripdata.zip\n",
      "Error downloading file status code: 202307-divvy-tripdata.zip, 200\n",
      "Requesting file: 202308-divvy-tripdata.zip\n",
      "Error downloading file status code: 202308-divvy-tripdata.zip, 200\n",
      "Requesting file: 202309-divvy-tripdata.zip\n",
      "Error downloading file status code: 202309-divvy-tripdata.zip, 200\n",
      "Requesting file: 202310-divvy-tripdata.zip\n",
      "Error downloading file status code: 202310-divvy-tripdata.zip, 200\n",
      "Requesting file: 202311-divvy-tripdata.zip\n",
      "Error downloading file status code: 202311-divvy-tripdata.zip, 200\n",
      "Requesting file: 202312-divvy-tripdata.zip\n",
      "Error downloading file status code: 202312-divvy-tripdata.zip, 200\n",
      "Unzipping files...\n",
      "No files to unzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 12:41:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Schema file ../data_collection/divvy-tripdata-schema.yaml does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m     load_dataset_to_local_fs(DATA_COLLECTION_DIR, [\u001B[38;5;241m2023\u001B[39m])\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# creating pyspark session and pre-processing data\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m divvy_df \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset_to_spark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_COLLECTION_DIR\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/TechProject1/divvy_bike_share_data_analysis/utils_pyspark.py:58\u001B[0m, in \u001B[0;36mload_dataset_to_spark\u001B[0;34m(data_path)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124;03mLoad data into spark session.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;03m:param data_path:\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124;03m:param spark_session:\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m local_spark_session \u001B[38;5;241m=\u001B[39m create_spark_session()\n\u001B[0;32m---> 58\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[43mload_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdivvy-tripdata-schema.yaml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m df \u001B[38;5;241m=\u001B[39m local_spark_session\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(data_path, schema\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[0;32m~/code/TechProject1/divvy_bike_share_data_analysis/utils_pyspark.py:48\u001B[0m, in \u001B[0;36mload_schema\u001B[0;34m(schema_path)\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m yaml\u001B[38;5;241m.\u001B[39mYAMLError(\n\u001B[1;32m     46\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myaml_error\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: Schema file ../data_collection/divvy-tripdata-schema.yaml does not exist."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading Pandas to validate some checks in sample data, to optimize time.\n",
    "\"\"\"\n",
    "\n",
    "from divvy_bike_share_data_analysis.data_loader import load_dataset_to_local_fs\n",
    "from divvy_bike_share_data_analysis.utils_pyspark import load_dataset_to_spark\n",
    "\n",
    "import glob\n",
    "\n",
    "# Collecting data to local directory\n",
    "DATA_COLLECTION_DIR: str = '../data_collection/'\n",
    "\n",
    "if not glob.glob(os.path.join(DATA_COLLECTION_DIR, '*.csv')):\n",
    "    print('collecting data to local fs')\n",
    "    # load_dataset_to_local_fs(DATA_COLLECTION_DIR, [2020, 2021, 2022, 2023])\n",
    "    # TODO notice to fact that 2020 and 2021, could affect our prediction due to the pandemic (context matters)\n",
    "    load_dataset_to_local_fs(DATA_COLLECTION_DIR, [2023])\n",
    "\n",
    "# creating pyspark session and pre-processing data\n",
    "divvy_df = load_dataset_to_spark(DATA_COLLECTION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling data for local validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b679d3feb5d664"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sampled_df = divvy_df.sample(withReplacement=False, fraction=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T20:50:39.422868Z",
     "start_time": "2024-02-05T20:50:39.298394Z"
    }
   },
   "id": "be85a1fd9a73562d",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* Verifying bike stations have correct name and id mapping for ech trip start and end stations\n",
    "* Remove Duplicate, trip records have Unique Trip Ids. Drop rows with null and duplicates values for `ride_id` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f5fdfbd50a60b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# removing duplicates and null values from ride_id and station Ids\n",
    "sampled_df = sampled_df.dropDuplicates(subset=['ride_id']).dropna(subset=['ride_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T20:50:39.484738Z",
     "start_time": "2024-02-05T20:50:39.349479Z"
    }
   },
   "id": "85aae3037e9f2ddb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat|start_lng|  end_lat|  end_lng|member_casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n",
      "|00005F98B137E17C|  docked_bike|2021-08-22 21:59:28|2021-08-22 23:12:35|Franklin St & Ill...|             RN-|Franklin St & Ill...|           RN-| 41.89102|-87.63548| 41.89102|-87.63548|       casual|\n",
      "|000062A9C95F1CA1| classic_bike|2022-10-28 13:31:38|2022-10-28 13:36:36| Morgan St & Polk St|    TA1307000130|Loomis St & Lexin...|         13332| 41.87174|-87.65103| 41.87219| -87.6615|       member|\n",
      "|0000D893A86A44A3|  docked_bike|2020-05-07 13:52:32|2020-05-07 14:04:45|Greenview Ave & F...|             188|Broadway & Barry Ave|           300|  41.9253| -87.6658|  41.9377| -87.6441|       member|\n",
      "|0000F5CDFCBDDEF2|  docked_bike|2020-10-13 19:14:24|2020-10-13 19:30:56|Wabash Ave & Gran...|             199|Halsted St & Will...|           224|41.891464|-87.62676|41.913864|-87.64876|       member|\n",
      "|0001EF8810240427|  docked_bike|2020-08-28 16:06:55|2020-08-28 16:36:24|Bissell St & Armi...|             113|Bissell St & Armi...|           113| 41.91844|-87.65222| 41.91844|-87.65222|       casual|\n",
      "|00021B5B147656B0| classic_bike|2021-09-13 08:00:08|2021-09-13 08:08:41|Loomis St & Lexin...|           13332|Halsted St & Polk St|  TA1307000121| 41.87219| -87.6615| 41.87184|-87.64664|       member|\n",
      "|00024513CED560E9|  docked_bike|2020-09-18 13:45:26|2020-09-18 13:59:14|Lincoln Ave & Ful...|             127|Lincoln Ave & Ros...|           230|41.925903|-87.64926| 41.94334|-87.67097|       member|\n",
      "|0003D25E0AB36FD8|electric_bike|2023-09-28 23:43:46|2023-09-28 23:51:04|McClurg Ct & Erie St|    KA1503000041|Wells St & Evergr...|  TA1308000049|41.894295|-87.61825|41.906723|-87.63483|       casual|\n",
      "|0005B4CD86764D59| classic_bike|2021-08-09 12:12:37|2021-08-09 12:27:00|Bissell St & Armi...|           13059|DuSable Lake Shor...|        LF-005| 41.91844|-87.65222| 41.91172| -87.6268|       member|\n",
      "|0005DDEB629E3BF0| classic_bike|2021-08-29 19:27:34|2021-08-29 19:48:43|Clark St & Wellin...|    TA1307000136|Sheridan Rd & Irv...|         13063|41.936497|-87.64754|41.954247| -87.6544|       casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# Get unique bike stations, removing rows with null values for station Ids\n",
    "from divvy_bike_share_data_analysis.bike_stations import get_unique_bike_stations_ids\n",
    "sampled_df = sampled_df.dropna(subset=['start_station_id', 'end_station_id'])\n",
    "sampled_df.show(10)\n",
    "bike_stations = get_unique_bike_stations_ids(sampled_df.select(['start_station_id',\n",
    "                                                        'start_station_name',\n",
    "                                                        'end_station_id',\n",
    "                                                        'end_station_name']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T20:56:25.822548Z",
     "start_time": "2024-02-05T20:50:39.480355Z"
    }
   },
   "id": "882d540e99328ac4",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying unified Station Names by ID back to sampled dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffea141e7eed4494"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (32 + 1) / 33]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat|start_lng|  end_lat|  end_lng|member_casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n",
      "|00005F98B137E17C|  docked_bike|2021-08-22 21:59:28|2021-08-22 23:12:35|Franklin St & Ill...|             RN-|Franklin St & Ill...|           RN-| 41.89102|-87.63548| 41.89102|-87.63548|       casual|\n",
      "|000062A9C95F1CA1| classic_bike|2022-10-28 13:31:38|2022-10-28 13:36:36| Morgan St & Polk St|    TA1307000130|Loomis St & Lexin...|         13332| 41.87174|-87.65103| 41.87219| -87.6615|       member|\n",
      "|0000D893A86A44A3|  docked_bike|2020-05-07 13:52:32|2020-05-07 14:04:45|Greenview Ave & F...|             188|Broadway & Barry Ave|           300|  41.9253| -87.6658|  41.9377| -87.6441|       member|\n",
      "|0000F5CDFCBDDEF2|  docked_bike|2020-10-13 19:14:24|2020-10-13 19:30:56|Wabash Ave & Gran...|             199|Halsted St & Will...|           224|41.891464|-87.62676|41.913864|-87.64876|       member|\n",
      "|0001EF8810240427|  docked_bike|2020-08-28 16:06:55|2020-08-28 16:36:24|Bissell St & Armi...|             113|Bissell St & Armi...|           113| 41.91844|-87.65222| 41.91844|-87.65222|       casual|\n",
      "|00021B5B147656B0| classic_bike|2021-09-13 08:00:08|2021-09-13 08:08:41|Loomis St & Lexin...|           13332|Halsted St & Polk St|  TA1307000121| 41.87219| -87.6615| 41.87184|-87.64664|       member|\n",
      "|00024513CED560E9|  docked_bike|2020-09-18 13:45:26|2020-09-18 13:59:14|Lincoln Ave & Ful...|             127|Lincoln Ave & Ros...|           230|41.925903|-87.64926| 41.94334|-87.67097|       member|\n",
      "|0003D25E0AB36FD8|electric_bike|2023-09-28 23:43:46|2023-09-28 23:51:04|McClurg Ct & Erie St|    KA1503000041|Wells St & Evergr...|  TA1308000049|41.894295|-87.61825|41.906723|-87.63483|       casual|\n",
      "|0005B4CD86764D59| classic_bike|2021-08-09 12:12:37|2021-08-09 12:27:00|Bissell St & Armi...|           13059|DuSable Lake Shor...|        LF-005| 41.91844|-87.65222| 41.91172| -87.6268|       member|\n",
      "|0005DDEB629E3BF0| classic_bike|2021-08-29 19:27:34|2021-08-29 19:48:43|Clark St & Wellin...|    TA1307000136|Sheridan Rd & Irv...|         13063|41.936497|-87.64754|41.954247| -87.6544|       casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+---------+---------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T20:59:56.340984Z",
     "start_time": "2024-02-05T20:57:00.946082Z"
    }
   },
   "id": "944fd4b9c0dddae3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from divvy_bike_share_data_analysis.bike_stations import categorize_time_of_day, categorize_day_of_week\n",
    "from pyspark.sql.functions import udf, hour, dayofweek\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# TODO refactoring to optmized queries and avoid duplicated columns after joins.\n",
    "# [Note to self] check bike_stations.get_unique_ids function for similar approach.\n",
    "\n",
    "# Prepare start and end stations DataFrames from bike_stations\n",
    "start_stations = bike_stations.selectExpr(\"station_id as start_station_id\", \"station_name as new_start_station_name\")\n",
    "end_stations = bike_stations.selectExpr(\"station_id as end_station_id\", \"station_name as new_end_station_name\")\n",
    "\n",
    "# Drop existing name columns in sampled_df before joining\n",
    "sampled_df = sampled_df.drop(\"start_station_name\", \"end_station_name\")\n",
    "\n",
    "# Join with start_stations to add new_start_station_name\n",
    "sampled_df = sampled_df.join(start_stations, on=\"start_station_id\", how=\"left\").withColumnRenamed(\"new_start_station_name\", \"start_station_name\")\n",
    "\n",
    "# Join with end_stations to add new_end_station_name\n",
    "sampled_df = sampled_df.join(end_stations, on=\"end_station_id\", how=\"left\").withColumnRenamed(\"new_end_station_name\", \"end_station_name\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T21:11:07.831526Z",
     "start_time": "2024-02-05T21:11:05.268442Z"
    }
   },
   "id": "6e82bb6f582d5bc4",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Adding new features \n",
    "\n",
    "As discussed in the [Technical Report](Reports/TechReport/out/Technical_Report_Diogo_Pessoa.pdf).\n",
    " \n",
    "I'll add a categorical field for time of day and day of week. Later I'll use these fields to analyze the most popular stations by time of day and day of week.\n",
    "For both Categorical fields the idea is to simplify the data and make it easier to analyze. While also labeling trips and opening an avenue for further exploration in regression models\n",
    "* Time of day: Morning, Afternoon, Evening, Night  \n",
    "* Day of week: Workday, non-Working"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84dc1c75f51649e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adding a categorical field for time of day and day of week.\n",
    "week_day_udf = udf(categorize_day_of_week, StringType())\n",
    "time_of_day_udf = udf(categorize_time_of_day, StringType())\n",
    "sampled_df_with_added_features = sampled_df.withColumn('day_period', time_of_day_udf(hour('started_at'))).withColumn(\n",
    "    'week_day', week_day_udf(dayofweek('started_at')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T21:11:20.546446Z",
     "start_time": "2024-02-05T21:11:19.311348Z"
    }
   },
   "id": "6f3cff4928a10ac7",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data exploration\n",
    "\n",
    "Section dedicated to exploring the data and validating the added features. Using Visualizations to understand the aggregated data and the relationships between the chosen features.\n",
    "\n",
    "#### Most Popular Start Stations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "698ddd32bb5a7810"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[start_station_name: string, count: bigint]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===============> (30 + 1) / 33][Stage 7:>                 (0 + 0) / 33]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Count Start stations\n",
    "# Group by 'start_station_id' and count\n",
    "station_counts = sampled_df_with_added_features.groupBy('start_station_name').count()\n",
    "print(station_counts) #TODO remove this print - debug only\n",
    "\n",
    "\"\"\"\n",
    "Converting to Pandas DataFrame to load into seaborn for plotting. \n",
    "Note I'm only converting to Pandas after the PySpark aggregations, aware of the performance implications in larger datasets. \n",
    "\"\"\"\n",
    "top_10_stations = station_counts.orderBy(desc('count')).limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-05T21:11:34.956607Z"
    }
   },
   "id": "14f029c3ee4fd0ba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17e0ef5389c983e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top_10_stations.head(10) #start stations\n",
    "# plot top ten stations, end. By day period. \n",
    "# Further explore the correlation start end, by day period, filtering by week day.\n",
    "# Plot a Correlation matrix of the features before applying PCA\n",
    "# TODO -  Stick to the fact that the goal in this project is to optimize the service to restock the stations, avoiding empty stations or users waiting to dock the bikes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T17:59:58.429591Z"
    }
   },
   "id": "8c467ef888c8348b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_10_stations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m8\u001B[39m))\n\u001B[1;32m      5\u001B[0m ax \u001B[38;5;241m=\u001B[39m sns\u001B[38;5;241m.\u001B[39mcountplot(\n\u001B[0;32m----> 6\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[43mtop_10_stations\u001B[49m,\n\u001B[1;32m      7\u001B[0m     x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_station_id\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      8\u001B[0m     palette\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviridis\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMost Popular Start Stations\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStart Station Names\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'top_10_stations' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x800 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.countplot(\n",
    "    data=top_10_stations,\n",
    "    x='start_station_id',\n",
    "    palette='viridis',\n",
    ")\n",
    "\n",
    "plt.title('Most Popular Start Stations')\n",
    "plt.xlabel('Start Station Names')\n",
    "plt.ylabel('Count By Trips')\n",
    "plt.xticks(rotation=45)  # Rotate the x-tick labels for better readability\n",
    "plt.show()\n",
    "# Save the plot to TechReport images folder\n",
    "plt.savefig(os.path.join(images_path, 'count_most_popular_start_stations.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:55:36.123837Z",
     "start_time": "2024-02-05T17:55:35.775010Z"
    }
   },
   "id": "5aff760300f9a7a3",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Unsupervised Learning] Predictive Analysis (Kmeans Clustering)\n",
    "\n",
    "Using the added features, I'll apply K-means clustering to find the most popular source stations by time of day.\n",
    "\n",
    "The steps involve: \n",
    "#### Data Preparation:\n",
    "##### Convert the categorical features to numeric index (StringIndexer)\n",
    "* Potentially use `OneHotEncoder`, since, we added the categorical fields for time of day and day of week, we may skip this step.\n",
    "* Vectorize the features (VectorAssembler)\n",
    "* Apply PCA to inspect the relationships between the chosen features, for correlation and variance.\n",
    " \n",
    "##### Clustering:\n",
    "\n",
    "Run an initial KMeans clustering with a default of k=5 clusters, then apply the silhouette index to look for the optimal number of clusters. \n",
    "Compare and plot results.\n",
    "\n",
    "Proceed with k-means for some specific questions:\n",
    "\n",
    "* Apply K-means clustering to find the most popular source/destinations stations - Plot  \n",
    "* Apply K-means for most popular stations by time of day/day of week - Plot \n",
    "* Most popular routes by day of week - Plot\n",
    "* Most popular routes by day of week/time of day - Plot\n",
    "\n",
    "In a real-world scenario, the period of the day (Morning, Afternoon, Evening, and Night) is not granular enough. To get better accuracy in the prediction, to confidently allocate, employees to relocate bikes. We should consider shorter time windows, increasing  granularity, such as 15-minute intervals.\n",
    "\n",
    "#### [Supervised Learning] Predictive Analysis (Regression)\n",
    "\n",
    "Explore a predictive analysis using regression models to improve user experience. Refining the re-stock strategy by considering the bike type per route, to predict the number of bikes needed at each station at different times of the day.\n",
    "\n",
    "* Apply regression models to predict the number of bikes needed at each station at different times of the day.\n",
    "* Bike type per routes, to predict the number of bikes needed at each station at different times of the day."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4333531c95e0b4d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# features = ['start_station_id', 'end_station_id', 'day_period', 'week_day']\n",
    "# # df = divvy_df_pandas[['start_station_id_index', 'end_station_id_index', 'day_period_index', 'week_day_index']]\n",
    "# df = pd.DataFrame(divvy_df_pandas[features], columns=features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d083e69e80d5793",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "352b9e80132c0bf3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Plotting correlation matrix of scaled set\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# corr = df.corr()\n",
    "# # Plot the heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='inferno', linewidth=.5, cbar=True, square=True)\n",
    "# plt.title('Feature Correlation Matrix')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aeffffe8c568ac3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Convert Station id to numeric Index Pandas \n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import pandas as pd\n",
    "# \n",
    "# le = LabelEncoder()\n",
    "# le.fit(divvy_df_pandas['start_station_id'])\n",
    "# divvy_df_pandas['start_station_id_index'] = le.transform(divvy_df_pandas['start_station_id'])\n",
    "# \n",
    "# # Convert end Station id to numeric Index Pandas \n",
    "# le.fit(divvy_df_pandas['end_station_id'])\n",
    "# divvy_df_pandas['end_station_id_index'] = le.transform(divvy_df_pandas['end_station_id'])\n",
    "# \n",
    "# # Convert day period to numeric index Pandas\n",
    "# le.fit(divvy_df_pandas['day_period'])\n",
    "# divvy_df_pandas['day_period_index'] = le.transform(divvy_df_pandas['day_period'])\n",
    "# \n",
    "# # Convert week day to numeric index Pandas\n",
    "# le.fit(divvy_df_pandas['week_day'])\n",
    "# divvy_df_pandas['week_day_index'] = le.transform(divvy_df_pandas['week_day'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7018c4a9ebd346f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # confusion matrix preparing to review the most popular stations by time of day. \n",
    "# # This is ahead of a PCA analysis the relationships between the chosen features.\n",
    "# \n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# # Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df)\n",
    "# \n",
    "# cm = confusion_matrix(features, df_scaled)\n",
    "# \n",
    "# # Plotting the confusion matrix\n",
    "# plt.figure(figsize=(10, 7))  # Adjust the size as needed\n",
    "# \n",
    "# # Using seaborn to create an annotated heatmap\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('Actual Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# \n",
    "# # Adjust tick labels if necessary\n",
    "# # plt.xticks(np.arange(len(your_tick_labels)), your_tick_labels, rotation=45)\n",
    "# # plt.yticks(np.arange(len(your_tick_labels)), your_tick_labels, rotation=45)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb18dd5a0f12bda9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# \n",
    "# import pandas as pd\n",
    "# \n",
    "# # Standardizing the features\n",
    "# \n",
    "# \n",
    "# \"\"\"\n",
    "# The goal here is to inspect the relationships between the chosen features, for correlation and variance.\n",
    "# PCA: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py \n",
    "# \"\"\"\n",
    "# # Applying PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# principalComponents = pca.fit_transform(df_scaled)\n",
    "# principalDf = pd.DataFrame(data=principalComponents, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "# \n",
    "# # Print results\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # Plotting\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(principalDf['Principal Component 1'], principalDf['Principal Component 2'])\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.title('2 component PCA')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92aa026c91faa146"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Repeat the Confusion Matrix with the PCA results\n",
    "# cm = confusion_matrix(principalDf['Principal Component 1'], principalDf['Principal Component 2'])\n",
    "# \n",
    "# # Plotting the confusion matrix\n",
    "# \n",
    "# plt.figure(figsize=(10, 7))  # Adjust the size as needed\n",
    "# \n",
    "# # Using seaborn to create an annotated heatmap\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('Actual Label')\n",
    "# plt.xlabel('Predicted Label')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "333d724d5762f53a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Run Starting Station KMeans Clustering based on day periods\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# \n",
    "# df = pd.DataFrame(divvy_df_pandas, columns=['start_station_id_index', 'day_period_index'])\n",
    "# \n",
    "# # I'll test scaling the features: and see the outcome\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df)\n",
    "# \n",
    "# kmeans = KMeans(n_clusters=11)\n",
    "# kmeans.fit(df)\n",
    "# df['start_station_cluster'] = kmeans.predict(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "294384e4683db2af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Plot kmeans results\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(data=divvy_df_pandas, x='start_station_id', y='day_period_index', hue='start_station_cluster',\n",
    "#                 palette='viridis')\n",
    "# plt.title('KMeans Clustering Results')\n",
    "# plt.xlabel('Start Station ID')\n",
    "# plt.xlabel('day_period_index')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b317606b8e0cde",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# \n",
    "# # After a default of k=5 clusters, I'll apply the silhouette index to look for the optimal number of clusters\n",
    "# \n",
    "# df = pd.DataFrame(divvy_df_pandas, columns=['start_station_id_index', 'day_period_index'])\n",
    "# \n",
    "# # I'll test scaling the features: and see the outcome\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df)\n",
    "# \n",
    "# k_values = range(2, 15)\n",
    "# silhouette_scores = []\n",
    "# \n",
    "# for k in k_values:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(df_scaled)\n",
    "# \n",
    "#     # Predict the cluster labels\n",
    "#     labels = kmeans.labels_\n",
    "# \n",
    "#     # Calculate the silhouette score and append it to the list\n",
    "#     silhouette_avg = silhouette_score(df_scaled, labels)\n",
    "#     silhouette_scores.append(silhouette_avg)\n",
    "#     print(f\"Silhouette Score for k={k}: {silhouette_avg}\")\n",
    "# \n",
    "# # Find the k with the highest silhouette score\n",
    "# optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "# print(f\"The optimal number of clusters k is: {optimal_k}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9fc465e537e2660",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(k_values, silhouette_scores, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Silhouette score')\n",
    "# plt.title('Silhouette score vs. Number of clusters (k)')\n",
    "# plt.xticks(np.arange(min(k_values), max(k_values) + 1, 1.0))\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# \n",
    "# # Output the optimal k based on silhouette score\n",
    "# optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "# print(f\"The optimal number of clusters k is: {optimal_k}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5299cffeed3a766",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# \n",
    "# indexer_start_station_id = StringIndexer(inputCol=\"start_station_id\", outputCol=\"start_station_id_index\")\n",
    "# indexer_end_station_id = StringIndexer(inputCol=\"end_station_id\", outputCol=\"end_station_id_index\")\n",
    "# sanitized_bike_stations_id_indexed = indexer_start_station_id.fit(sanitized_bike_stations).transform(\n",
    "#     sanitized_bike_stations)\n",
    "# sanitized_bike_stations_id_indexed = indexer_end_station_id.fit(sanitized_bike_stations).transform(\n",
    "#     sanitized_bike_stations_id_indexed)\n",
    "# sanitized_bike_stations_id_indexed.show()\n",
    "# sanitized_bike_stations = sampled_df_with_day_period_indexed.join(\n",
    "#     bike_stations.withColumnRenamed(\"station_name\", \"start_station_name\"),\n",
    "#     sampled_df_with_day_period_indexed.start_station_id == bike_stations.station_id,\n",
    "#     \"left\").drop(\"station_id\")\n",
    "# \n",
    "# # Join to map end_station_id to end_station_name\n",
    "# sanitized_bike_stations = sanitized_bike_stations.join(\n",
    "#     bike_stations.withColumnRenamed(\"station_name\", \"end_station_name\"),\n",
    "#     sanitized_bike_stations.end_station_id == bike_stations.station_id,\n",
    "#     \"left\").drop(\"station_id\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b005a367f1496da8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24109f00d6906fd0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO Erroring out, need to fix this\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# \n",
    "# # most popular stations by period of day\n",
    "# feature_cols = ['start_station_id_index', 'day_period_index']\n",
    "# assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "# \n",
    "# # Transform the data to have the features column\n",
    "# adf_kmeans = assembler.transform(sanitized_bike_stations_id_indexed)\n",
    "# ### Custom spark Session to support synapsesML:\n",
    "# import pyspark\n",
    "# \n",
    "# spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.2\") \\\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "#     .getOrCreate()\n",
    "# \n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# \n",
    "# # Number of clusters\n",
    "# k = 5  # Adjust based on your data and needs\n",
    "# \n",
    "# kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "# model = kmeans.fit(adf_kmeans)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0fe4bd6b32b3c08",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Make predictions\n",
    "# # predictions = model.transform(adf_kmeans)\n",
    "# # \n",
    "# # # Group by cluster label and start_station_id, and count occurrences\n",
    "# # popular_start_stations = predictions.groupBy(\"prediction\", \"start_station_id_index\")\n",
    "# # \n",
    "# # # Find the most popular station in each cluster\n",
    "# # most_popular_start_stations = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "# # \n",
    "# # popular_destinations = predictions.groupBy(\"prediction\", \"end_station_id_index\").count()\n",
    "# # most_popular_destination = popular_start_stations.groupBy(\"prediction\").max(\"count\")\n",
    "# \n",
    "# # TODO apply K-means clustering and to find the most popular source stations - Plot\n",
    "# # TODO - Can I apply K-means for most popular stations by time of day? - Plot\n",
    "# # TODO - most popular routes - Plot \n",
    "# \n",
    "# \n",
    "# predictions.columns\n",
    "# \n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(data=predictions, x='start_station_id_index', y='start_lat', hue='prediction', palette='viridis')\n",
    "# # plt.title('KMeans Clustering Results')\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c36964280c62938c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Will use this confusion Matrix when I get to the Categorical analysis of the data. \n",
    "# \"\"\"\n",
    "# # confusion matrix preparing to review the most popular stations by time of day. \n",
    "# # This is ahead of a PCA analysis the relationships between the chosen features.\n",
    "# \n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# # Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df)\n",
    "# \n",
    "# cm = confusion_matrix(features, df_scaled)\n",
    "# \n",
    "# # Plotting the confusion matrix\n",
    "# plt.figure(figsize=(10, 7))  # Adjust the size as needed\n",
    "# \n",
    "# # Using seaborn to create an annotated heatmap\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('Actual Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# \n",
    "# # Adjust tick labels if necessary\n",
    "# # plt.xticks(np.arange(len(your_tick_labels)), your_tick_labels, rotation=45)\n",
    "# # plt.yticks(np.arange(len(your_tick_labels)), your_tick_labels, rotation=45)\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53d4906838cb017"
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
