{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## [Unsupervised Learning] Predictive Analysis (Kmeans Clustering)\n",
    "\n",
    "Using the added features, I'll apply K-means clustering to find the most popular source stations by time of day.\n",
    "##### Clustering:\n",
    "\n",
    "Run an initial KMeans clustering with a default of k=5 clusters, then apply the silhouette index to look for the optimal number of clusters.\n",
    "Compare and plot results.\n",
    "\n",
    "Proceed with k-means for some specific questions:\n",
    "\n",
    "* Apply K-means clustering to find the most popular source/destinations stations - Plot\n",
    "* Apply K-means for most popular stations by time of day/day of week - Plot\n",
    "* Most popular routes by day of week - Plot\n",
    "* Most popular routes by day of week/time of day - Plot\n",
    "\n",
    "In a real-world scenario, the period of the day (Morning, Afternoon, Evening, and Night) is not granular enough. To get better accuracy in the prediction, to confidently allocate, employees to relocate bikes. We should consider shorter time windows, increasing  granularity, such as 15-minute intervals.\n",
    "\n",
    "#### [Supervised Learning] Predictive Analysis (Regression)\n",
    "\n",
    "Explore a predictive analysis using regression models to improve user experience. Refining the re-stock strategy by considering the bike type per route, to predict the number of bikes needed at each station at different times of the day.\n",
    "\n",
    "* Apply regression models to predict the number of bikes needed at each station at different times of the day.\n",
    "* Bike type per routes, to predict the number of bikes needed at each station at different times of the day.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['start_station_id_index', 'end_station_id_index', 'day_period_index', 'week_day_index']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# # Scaling the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# # Combine the VectorAssembler and StandardScaler into a Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# You can now define a pipeline that includes both the assembler and the scaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit and transform the DataFrame using the defined pipeline\n",
    "sampled_df_scaled = pipeline.fit(sampled_df_with_added_features_indexed).transform(\n",
    "    sampled_df_with_added_features_indexed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "adf_kmeans = sampled_df_scaled\n",
    "# Initialize KMeans with the specified number of clusters (k) and a seed for reproducibility\n",
    "kmeans = KMeans().setK(5).setSeed(1).setFeaturesCol(\"features\")\n",
    "\n",
    "# Fit the model to the data\n",
    "model = kmeans.fit(adf_kmeans)\n",
    "\n",
    "# Transform the dataset to include cluster predictions\n",
    "predictions = model.transform(adf_kmeans)\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize the evaluator with silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Evaluate the model\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score with k={k}: {silhouette}\")\n",
    "# Silhouette Score with k=5: 0.5733339010164845"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO apply K-means clustering and to find the most popular source stations - Plot\n",
    "# TODO - Can I apply K-means for most popular stations by time of day? - Plot\n",
    "# TODO - most popular routes - Plot\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression is a statistical method commonly used for binary classification problems, where the goal is to predict the probability that a given input belongs to a particular class. However, Logistic Regression can also be extended to handle multi-class classification problems, where you have more than two classes to predict. This extension is typically achieved through one of two strategies: One-vs-Rest (OvR) and Multinomial Logistic Regression (also known as Softmax Regression).\n",
    "\n",
    "### One-vs-Rest (OvR)\n",
    "\n",
    "One-vs-Rest, also known as One-vs-All, is a strategy for handling multi-class classification problems with Logistic Regression by training a separate binary classifier for each class. For a problem with \\(N\\) classes, \\(N\\) separate Logistic Regression models are trained. Each model predicts the probability that a given input belongs to one of the classes versus all other classes combined. When making predictions, the class that is predicted with the highest probability by its respective model is chosen as the final output.\n",
    "\n",
    "### Multinomial Logistic Regression (Softmax Regression)\n",
    "\n",
    "Multinomial Logistic Regression is a direct extension of binary Logistic Regression to multi-class problems, without needing to train multiple binary classifiers. Instead of using the sigmoid function (which outputs probabilities between 0 and 1) for the binary case, Multinomial Logistic Regression uses the softmax function to handle multiple classes. The softmax function takes the raw model outputs (called logits) for each class and converts them into probabilities by taking the exponential of each output and then normalizing these values by dividing by the sum of all the exponentials. This ensures that the output probabilities for all classes sum up to 1.\n",
    "\n",
    "The model then predicts the class with the highest probability.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a sample \\(x\\), the probability \\(P(y=j|x)\\) that \\(x\\) belongs to class \\(j\\) can be modeled in the Multinomial Logistic Regression as:\n",
    "\n",
    "\\[ P(y=j|x) = \\frac{e^{(w_j \\cdot x + b_j)}}{\\sum_{k=1}^{K} e^{(w_k \\cdot x + b_k)}} \\]\n",
    "\n",
    "where \\(w_j\\) and \\(b_j\\) are the weight and bias parameters for class \\(j\\), and \\(K\\) is the total number of classes. The model is trained to optimize these parameters to minimize the discrepancy between the predicted probabilities and the actual class labels, typically using a cost function like cross-entropy loss.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Simple and easy to implement.\n",
    "- Efficient to train.\n",
    "- Provides probabilities for predictions, offering insight into the model's confidence.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Assumes linear boundaries between classes, which might not always be the case.\n",
    "- Can struggle with complex relationships or high-dimensional data without feature engineering or regularization.\n",
    "\n",
    "In practice, Logistic Regression can serve as a strong baseline for multi-class classification problems, but its effectiveness can depend on the nature of the dataset and how well the assumptions of Logistic Regression align with the data's characteristics.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
